# -*- coding: utf-8 -*-
# ãƒ¡ã‚¤ãƒ³ç”»é¢ä¸­å¿ƒUI + æ¨©é™ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«/ãƒ“ãƒ¥ãƒ¼è‡ªå‹•ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆSHOWæœªä½¿ç”¨ï¼‰ + ä½“æ„Ÿæ€§èƒ½æ”¹å–„ç‰ˆ
# æ–¹é‡ï¼š
#  - Information Schema ã ã‘ã§æ¨©é™ã‚’è§£æ±ºï¼ˆENABLED_ROLES / OBJECT_PRIVILEGESï¼‰
#  - ãƒ“ãƒ¥ãƒ¼ã‚’å«ã‚ã‚‹ã€ãƒ­ãƒ¼ãƒ«ç¶™æ‰¿ã‚’å«ã‚€ã€ALLOWED_TABLESã¯ä½¿ã‚ãªã„
#  - UIã¯ã€Œã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼ + ã‚¿ãƒ– + 2ã‚«ãƒ©ãƒ ï¼ˆè¨­å®š/çµæœï¼‰ã€ã§ä¸€ç”»é¢å®Œçµ
#  - å½“é¢ã¯ TEST_DB.TEST å›ºå®šã€‚å°†æ¥ã¯ TARGETS ã‚’å¢—ã‚„ã™ã ã‘ã§æ‹¡å¼µå¯èƒ½ã€‚

import streamlit as st
import pandas as pd
import datetime
import re
from io import BytesIO
import snowflake.connector
import zipfile
import csv
import io

# -------------------------------------------------
# ãƒšãƒ¼ã‚¸è¨­å®š
# -------------------------------------------------
st.set_page_config(page_title="ãƒ‡ãƒ¼ã‚¿é–²è¦§", layout="wide")

# è»½é‡ãƒ†ãƒ¼ãƒï¼ˆé…è‰²ãƒ»ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºï¼‰
st.markdown("""
<style>
:root {
  --accent: #2563EB;     /* ãƒœã‚¿ãƒ³ç­‰ã®ã‚¢ã‚¯ã‚»ãƒ³ãƒˆ */
  --ok: #16A34A;         /* æˆåŠŸ */
  --warn: #D97706;       /* è­¦å‘Š */
  --muted: #6B7280;      /* ã‚µãƒ–ãƒ†ã‚­ã‚¹ãƒˆ */
}
.block-container { padding-top: 1rem; }
h1, h2, h3 { font-weight: 600; }
div[data-testid="stMarkdownContainer"] code, pre code { font-size: 0.95rem; }
div.stButton > button[kind="primary"] {
  background: var(--accent); border-color: var(--accent);
}
div.stButton > button[disabled] { opacity: 0.5; cursor: not-allowed; }
div.stTabs [data-baseweb="tab"] { font-size: 0.95rem; }
.small-muted { color: var(--muted); font-size: 0.9rem; }
.badge { display:inline-block; padding: 0.1rem .5rem; border-radius: .4rem; font-size:.8rem; }
.badge-ok { background:#DCFCE7; color:#166534; }
.badge-warn { background:#FEF9C3; color:#854D0E; }
.badge-run { background:#DBEAFE; color:#1E40AF; }
hr { margin: .8rem 0; }
</style>
""", unsafe_allow_html=True)

# -------------------------------------------------
# è¨­å®šï¼ˆæ‹¡å¼µå¯èƒ½ãªå¯¾è±¡ã‚¹ã‚­ãƒ¼ãƒï¼‰
# -------------------------------------------------
TARGETS = [
    {"db": "TEST_DB", "schema": "TEST"},
]

# -------------------------------------------------
# å®šæ•°
# -------------------------------------------------
DELIMITER_COMMA = ','
DELIMITER_TAB = '\t'
STAGE_NAME = '@test_s3_stage'
S3_DirName = '/test/'
S3_FileName = 'testfilename'
CSV_MAX = 50000   # CSV/TSV ã®ZIPåˆ†å‰²è¡Œæ•°
EXCEL_MAX = 50000 # Excelã®æœ€å¤§è¡Œæ•°

# -------------------------------------------------
# æ¥ç¶šãƒ»å…±é€šã‚¯ã‚¨ãƒªé–¢æ•°
# -------------------------------------------------
@st.cache_resource
def get_conn():
    """Snowflakeæ¥ç¶šã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§å†åˆ©ç”¨"""
    # å¿…è¦ã«å¿œã˜ã¦æ¥ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„
    # ä¾‹:
    # return snowflake.connector.connect(
    #     account="xxx",
    #     user="xxx",
    #     password="xxx",
    #     warehouse="xxx",
    #     role="xxx",
    #     database="xxx",
    #     schema="xxx",
    # )
    return snowflake.connector.connect()

def _normalize_params(params):
    """executeã«æ¸¡ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ï¼ˆç©ºdictã‚’æ¸¡ã•ãªã„ï¼š252004å¯¾ç­–ï¼‰"""
    if params is None:
        return None
    if isinstance(params, (list, tuple)):
        return params if len(params) > 0 else None
    if isinstance(params, dict):
        return list(params.values()) if len(params) > 0 else None
    return [params]

@st.cache_data(ttl=600)
def run_query(sql: str, params=None) -> pd.DataFrame:
    """
    Snowflakeã«SQLã‚’æŠ•ã’ã¦DataFrameã‚’è¿”ã™ã€‚
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç„¡ã—ã¯ None ã‚’æ¸¡ã—ã€ç©ºdictã¯æ¸¡ã•ãªã„ï¼ˆ252004å¯¾ç­–ï¼‰ã€‚
    """
    with get_conn().cursor() as cur:
        cur.execute(sql, _normalize_params(params))
        try:
            return cur.fetch_pandas_all()  # Arrowãƒ™ãƒ¼ã‚¹ã§é«˜é€Ÿ
        except Exception:
            rows = cur.fetchall()
            cols = [c[0] for c in cur.description]
            return pd.DataFrame(rows, columns=cols)

# -------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------------------------------
@st.cache_data(ttl=3600)
def get_identity():
    acc = run_query("SELECT CURRENT_ACCOUNT() AS ACCOUNTNAME")
    usr = run_query("SELECT CURRENT_USER() AS USERNAME")
    return acc["ACCOUNTNAME"][0], usr["USERNAME"][0]

def sanitize_ident(s: str) -> str:
    """è­˜åˆ¥å­ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆè‹±æ•°ï¼‹ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰"""
    return re.sub(r"[^A-Za-z0-9_]", "", s or "")

# -------------------------------------------------
# SHOW éä¾å­˜ã®ãƒ­ãƒ¼ãƒ«ï¼æ¨©é™è§£æ±ºï¼ˆInformation Schemaï¼‰
# -------------------------------------------------
@st.cache_data(ttl=300)
def get_enabled_roles(target_db: str = "TEST_DB") -> list[str]:
    """
    ç¾ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æœ‰åŠ¹ï¼ˆç¶™æ‰¿å«ã‚€ï¼‰ãªãƒ­ãƒ¼ãƒ«ã€‚
    ENABLED_ROLES ã¯å„DBã® INFORMATION_SCHEMA ã«ã‚ã‚‹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ãƒ“ãƒ¥ãƒ¼ã€‚
    """
    target_db = sanitize_ident(target_db)
    df = run_query(f"SELECT ROLE_NAME FROM {target_db}.INFORMATION_SCHEMA.ENABLED_ROLES")
    return df["ROLE_NAME"].tolist() if not df.empty else []

@st.cache_data(ttl=300)
def get_effective_select_objects(
    target_db: str,
    target_schema: str,
    include_views: bool = True,
    include_materialized_views: bool = False
) -> list[str]:
    """
    Information Schemaã®ã¿ã§åˆ¤å®šï¼š
      - æœ‰åŠ¹ãƒ­ãƒ¼ãƒ«: ENABLED_ROLES
      - æ¨©é™     : OBJECT_PRIVILEGES (SELECT / OWNERSHIP)
    å¯¾è±¡DB/ã‚¹ã‚­ãƒ¼ãƒå†…ã§ SELECT å¯èƒ½ãª TABLE/VIEWï¼ˆï¼‹ä»»æ„ã§ MATERIALIZED VIEWï¼‰åã‚’è¿”ã™ã€‚
    """
    target_db = sanitize_ident(target_db)
    target_schema = sanitize_ident(target_schema)

    roles = get_enabled_roles(target_db)
    if not roles:
        return []

    obj_types = ["TABLE"]
    if include_views:
        obj_types.append("VIEW")
    if include_materialized_views:
        obj_types.append("MATERIALIZED VIEW")

    obj_types_sql = ", ".join(f"'{t}'" for t in obj_types)
    roles_sql = ", ".join(f"'{r}'" for r in roles)

    q = f"""
        SELECT DISTINCT OBJECT_NAME
        FROM {target_db}.INFORMATION_SCHEMA.OBJECT_PRIVILEGES
        WHERE OBJECT_SCHEMA = '{target_schema}'
          AND OBJECT_TYPE IN ({obj_types_sql})
          AND PRIVILEGE_TYPE IN ('SELECT','OWNERSHIP')
          AND GRANTEE IN ({roles_sql})
        ORDER BY OBJECT_NAME
    """
    df = run_query(q)
    return df["OBJECT_NAME"].tolist() if not df.empty else []

@st.cache_data(ttl=300)
def get_allowed_tables() -> list[str]:
    """
    æ¨©é™ãƒ™ãƒ¼ã‚¹ã§å‚ç…§å¯èƒ½ãª TABLE/VIEW ã®ä¸€è¦§ã€‚
    å°†æ¥ TARGETS ã«è¤‡æ•°ã‚¹ã‚­ãƒ¼ãƒã‚’ä¸¦ã¹ãŸå ´åˆã¯å’Œé›†åˆã‚’è¿”ã™ã€‚
    """
    all_effective = set()
    for t in TARGETS:
        objs = get_effective_select_objects(
            target_db=t["db"], target_schema=t["schema"],
            include_views=True, include_materialized_views=False
        )
        all_effective.update(objs)
    return sorted(all_effective)

# -------------------------------------------------
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ / S3 / ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
# -------------------------------------------------
def to_excel_bytes(df: pd.DataFrame) -> bytes:
    output = BytesIO()
    df_to_save = df.copy()
    for col in df_to_save.select_dtypes(include=['datetimetz']).columns:
        df_to_save[col] = df_to_save[col].dt.tz_localize(None)
    try:
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            df_to_save.to_excel(writer, index=False, sheet_name="ãƒ‡ãƒ¼ã‚¿")
    except Exception:
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_to_save.to_excel(writer, index=False, sheet_name="ãƒ‡ãƒ¼ã‚¿")
    return output.getvalue()

def generate_download(df: pd.DataFrame, filetype: str = "csv", quote_option='"', split_limit: int = CSV_MAX):
    if filetype in ["csv", "tsv"]:
        sep = "\t" if filetype == "tsv" else ","
        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
            for i, start in enumerate(range(0, len(df), split_limit)):
                part = df.iloc[start:start + split_limit].copy()
                output = io.StringIO()
                if quote_option == 'ãªã—':
                    part.to_csv(output, index=False, sep=sep, quoting=csv.QUOTE_NONE, escapechar='\\')
                else:
                    part.to_csv(output, index=False, sep=sep, quotechar=quote_option, quoting=csv.QUOTE_ALL)
                filename = f"part{i + 1}.{filetype}"
                zf.writestr(filename, output.getvalue().encode("utf-8"))
        zip_buffer.seek(0)
        return zip_buffer
    elif filetype == "excel":
        if len(df) > EXCEL_MAX:
            return None
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="Sheet1")
        return output.getvalue()

def stream_query_to_zip(sql: str, sep: str = ",", quotechar: str = '"', split_limit: int = CSV_MAX) -> io.BytesIO:
    """
    Snowflake -> fetchmany -> é€æ¬¡CSVæ›¸ãå‡ºã— -> ZIPï¼ˆåˆ†å‰²ï¼‰
    DataFrameã‚’çµŒç”±ã—ãªã„ãŸã‚é«˜é€Ÿãƒ»çœãƒ¡ãƒ¢ãƒª
    """
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        with get_conn().cursor() as cur:
            cur.execute(sql)
            cols = [c[0] for c in cur.description]
            part_no, written = 1, 0
            out = io.StringIO()
            writer = csv.writer(out, delimiter=sep, quotechar=quotechar, quoting=csv.QUOTE_ALL)
            writer.writerow(cols)
            while True:
                rows = cur.fetchmany(10_000)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯èª¿æ•´å¯
                if not rows:
                    break
                for row in rows:
                    if written and written % split_limit == 0:
                        zf.writestr(f"part{part_no}.csv", out.getvalue().encode("utf-8"))
                        part_no += 1
                        out = io.StringIO()
                        writer = csv.writer(out, delimiter=sep, quotechar=quotechar, quoting=csv.QUOTE_ALL)
                        writer.writerow(cols)
                    writer.writerow(row)
                    written += 1
            if written == 0:
                zf.writestr("part1.csv", out.getvalue().encode("utf-8"))
            else:
                zf.writestr(f"part{part_no}.csv", out.getvalue().encode("utf-8"))
    buf.seek(0)
    return buf

def S3_upload(query: str, delimiter: str, filename: str):
    with st.spinner("S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
        run_query(f"""
COPY INTO {STAGE_NAME}{S3_DirName}{S3_FileName}{filename}
FROM ({query})
FILE_FORMAT = (
  TYPE = CSV
  FIELD_DELIMITER = '{delimiter}'
  FIELD_OPTIONALLY_ENCLOSED_BY = '\"'
  COMPRESSION = GZIP
  HEADER = TRUE
)
OVERWRITE = TRUE
SINGLE = TRUE;
""")
    st.success("S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ã—ã¾ã—ãŸ")

def show_download_ui(df: pd.DataFrame, table_name: str, key_prefix: str = "download"):
    if df.empty:
        st.warning("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    today_str = datetime.date.today().strftime("%Y%m%d")
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        quote_option = st.selectbox("å›²ã„æ–‡å­—", ['"', "'", 'ãªã—'], index=0, key=f"{key_prefix}_quote")
    with col2:
        ft = st.selectbox("å½¢å¼", ["csv", "tsv", "excel"], index=0, key=f"{key_prefix}_fmt")
    with col3:
        if ft in ("csv","tsv"):
            data = generate_download(df, filetype=ft, quote_option=quote_option)
            st.download_button(
                label="ğŸ“¥ ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=data,
                file_name=f"{table_name}_{today_str}_{ft.upper()}.zip",
                mime="application/zip",
                key=f"{key_prefix}_dlzip"
            )
        else:
            data = generate_download(df, filetype="excel")
            if data:
                st.download_button(
                    label="ğŸ“¥ Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=data,
                    file_name=f"{table_name}_{today_str}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"{key_prefix}_xldl"
                )
            else:
                st.warning("Excelã¯50,000ä»¶ã‚’è¶…ãˆã‚‹ãŸã‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚")

def download_ready_ui(df_preview: pd.DataFrame, table_name: str, sql_all_func):
    """
    å¾“æ¥ã®ã€Œå…¨ä»¶ã‚’DFã«ç©ã‚“ã§ã‹ã‚‰DLã€ï¼‹ é«˜é€Ÿã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°DLï¼ˆDFä¸è¦ï¼‰ã®ä¸¡å¯¾å¿œã€‚
    """
    if df_preview.empty:
        st.warning("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    colA, colB, colC = st.columns([1.2, 1.2, 2])
    with colA:
        if st.button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™ï¼ˆå…¨ä»¶å–å¾—ï¼‰"):
            with st.spinner("å…¨ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                st.session_state.df_for_download = sql_all_func()
            st.success(f"å…¨ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ ({len(st.session_state.df_for_download)}ä»¶)")
    with colB:
        sep_choice = st.radio("åŒºåˆ‡ã‚Š", ["CSV", "TSV"], horizontal=True, key="fast_sep")
        sep = "\t" if sep_choice == "TSV" else ","
        quote_option = st.selectbox("å›²ã„æ–‡å­—", ['"', "'"], index=0, key="fast_quote")
        if st.button("ğŸ“¥ é«˜é€ŸZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"):
            with st.spinner("ZIPç”Ÿæˆä¸­..."):
                data = stream_query_to_zip(st.session_state.get("last_query",""), sep=sep, quotechar=quote_option)
            st.download_button(
                "ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹",
                data=data,
                file_name=f"{table_name}_{datetime.date.today():%Y%m%d}_server.zip",
                mime="application/zip",
                key="fast_zip_dl"
            )
    with colC:
        st.caption("S3 ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        c1, c2 = st.columns(2)
        with c1:
            if st.button("â¤´ CSVã‚’S3ã¸"):
                S3_upload(st.session_state.get("last_query",""), DELIMITER_COMMA, "_data_CSV")
        with c2:
            if st.button("â¤´ TSVã‚’S3ã¸"):
                S3_upload(st.session_state.get("last_query",""), DELIMITER_TAB, "_data_TSV")

# -------------------------------------------------
# SQLæ•´å½¢
# -------------------------------------------------
_LIMIT_PATTERN = re.compile(r"(?i)LIMIT\\s+\\d+")

def remove_limit(sql: str) -> str:
    return _LIMIT_PATTERN.sub("", sql).strip()

def clean_sql(sql: str) -> str:
    if not sql:
        return ""
    return sql.strip().rstrip(";")

def get_full_data() -> pd.DataFrame:
    sql = st.session_state.get("last_query", "")
    if not sql:
        st.warning("å®Ÿè¡Œå¯¾è±¡ã®SQLãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()
    sql_full = clean_sql(remove_limit(sql))
    return run_query(sql_full)

# -------------------------------------------------
# SQLä¿å­˜
# -------------------------------------------------
def save_sql_to_log(user: str, sql_text: str, sql_name: str):
    """
    SQLã‚’ {TARGETS[0]} ã® SQL_LOG ã«ä¿å­˜ã€‚å°†æ¥ã¯æ ¼ç´å…ˆã‚‚å¯å¤‰ã«ã§ãã¾ã™ã€‚
    """
    if not sql_name or not sql_name.strip():
        st.warning("ä¿å­˜åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return
    target_db = sanitize_ident(TARGETS[0]["db"])
    target_schema = sanitize_ident(TARGETS[0]["schema"])
    try:
        sql_to_save = sql_text.replace("'", "''")
        name_to_save = sql_name.replace("'", "''")
        insert_sql = f"""
INSERT INTO {target_db}.{target_schema}.sql_log (user_ID, sql_name, exec_query, save_date)
VALUES ('{user}', '{name_to_save}', '{sql_to_save}', CURRENT_TIMESTAMP)
"""
        with get_conn().cursor() as cur:
            cur.execute(insert_sql)
            get_conn().commit()
        st.session_state["sql_saved_message"] = f"SQLã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ï¼ˆ{sql_name}ï¼‰"
    except Exception as e:
        st.session_state["sql_saved_message"] = f"ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"

def show_sql_save_message():
    if "sql_saved_message" in st.session_state:
        st.success(st.session_state["sql_saved_message"])
        del st.session_state["sql_saved_message"]

# -------------------------------------------------
# ç”»é¢ä¸Šéƒ¨ï¼šãƒ˜ãƒƒãƒ€ / ã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼ / ã‚¿ãƒ–
# -------------------------------------------------
current_account, current_user = get_identity()

st.markdown(f"### ğŸ“Š ãƒ‡ãƒ¼ã‚¿é–²è¦§ & ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
st.markdown(
    f"<span class='small-muted'>ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ: <b>{current_account}</b> / ãƒ¦ãƒ¼ã‚¶ãƒ¼: <b>{current_user}</b></span>",
    unsafe_allow_html=True
)

# ã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼ï¼ˆDB/ã‚¹ã‚­ãƒ¼ãƒã¯å½“é¢å›ºå®šï¼‰
target_db = sanitize_ident(TARGETS[0]["db"])
target_schema = sanitize_ident(TARGETS[0]["schema"])

# æ¨©é™ãƒ™ãƒ¼ã‚¹ã®å€™è£œä¸€è¦§
all_tables = get_allowed_tables()

cb_left, cb_mid, cb_right = st.columns([1.2, 2.4, 2])
with cb_left:
    st.markdown(f"**å¯¾è±¡**: `{target_db}.{target_schema}`")
with cb_mid:
    selected_table = st.selectbox("ãƒ†ãƒ¼ãƒ–ãƒ«/ãƒ“ãƒ¥ãƒ¼ã‚’é¸æŠ", all_tables, index=0 if all_tables else None, placeholder="é¸æŠã—ã¦ãã ã•ã„")
with cb_right:
    col_exec, col_save, col_dl = st.columns([1, 1, 1])
    with col_exec:
        exec_clicked = st.button("å®Ÿè¡Œ â–¶", type="primary", help="ç›´è¿‘ã®è¨­å®šã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å†å®Ÿè¡Œ")
    with col_save:
        save_clicked = st.button("ğŸ’¾ ä¿å­˜")
    with col_dl:
        dl_clicked = st.button("ğŸ“¥ DLæº–å‚™")

st.markdown("<hr/>", unsafe_allow_html=True)
tabs = st.tabs(["â‘  ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ & ãƒ•ã‚£ãƒ«ã‚¿", "â‘¡ è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«çµåˆ", "â‘¢ ä¿å­˜ã—ãŸSQL", "â‘£ ãƒ¡ã‚¿æƒ…å ±"])

# -------------------------------------------------
# â‘  ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ & ãƒ•ã‚£ãƒ«ã‚¿ ã‚¿ãƒ–
# -------------------------------------------------
with tabs[0]:
    if not selected_table:
        st.info("ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼ã§ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ãƒ“ãƒ¥ãƒ¼ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
    else:
        left, right = st.columns([1, 1.6])

        # ã‚«ãƒ©ãƒ æƒ…å ±
        tname = sanitize_ident(selected_table)
        df_columns = run_query(f"""
            SELECT COLUMN_NAME, DATA_TYPE, COMMENT
            FROM {target_db}.INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA='{target_schema}' AND TABLE_NAME='{tname}'
            ORDER BY ORDINAL_POSITION
        """)
        columns = df_columns["COLUMN_NAME"].tolist()

        with left:
            st.subheader("è¨­å®š")
            with st.form(key="filter_form", clear_on_submit=False):
                default_cols = st.session_state.get("selected_columns", columns[: min(5, len(columns))])
                selected_columns = st.multiselect("è¡¨ç¤ºã™ã‚‹ã‚«ãƒ©ãƒ ", columns, default=default_cols)

                # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆç°¡æ˜“ï¼‰
                st.markdown("**åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰**")
                basic_filters = []
                for col in selected_columns:
                    val = st.text_input(f"{col}", key=f"like_{col}")
                    if val.strip():
                        basic_filters.append(f'"{col}" LIKE \'%{val.strip()}%\'')

                # è©³ç´°ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
                with st.expander("è©³ç´°æ¡ä»¶ï¼ˆæ•°å€¤ãƒ»æ—¥ä»˜ãªã©ï¼‰", expanded=False):
                    adv_filters = []
                    for col in selected_columns:
                        dtype = df_columns.loc[df_columns["COLUMN_NAME"] == col, "DATA_TYPE"].iloc[0].upper()
                        if any(t in dtype for t in ["NUMBER", "INT", "FLOAT", "DECIMAL", "DOUBLE"]):
                            c1, c2 = st.columns(2)
                            with c1:
                                minv = st.text_input(f"{col} æœ€å°å€¤", key=f"min_{col}")
                            with c2:
                                maxv = st.text_input(f"{col} æœ€å¤§å€¤", key=f"max_{col}")
                            if minv.strip(): adv_filters.append(f'"{col}" >= {minv.strip()}')
                            if maxv.strip(): adv_filters.append(f'"{col}" <= {maxv.strip()}')
                        elif "DATE" in dtype or "TIME" in dtype:
                            c1, c2 = st.columns(2)
                            with c1:
                                date_from = st.date_input(f"{col} ä»¥é™", value=None, key=f"from_{col}")
                            with c2:
                                date_to = st.date_input(f"{col} ä»¥å‰", value=None, key=f"to_{col}")
                            if date_from: adv_filters.append(f'"{col}" >= \'{date_from}\'')
                            if date_to:   adv_filters.append(f'"{col}" <= \'{date_to}\'')

                submitted = st.form_submit_button("é©ç”¨ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼100ä»¶ï¼‰", type="primary")
                if submitted:
                    st.session_state.selected_columns = selected_columns
                    clauses = basic_filters + adv_filters
                    where_sql = f"WHERE {' AND '.join(clauses)}" if clauses else ""
                    quoted_cols = [f'"{c}"' for c in selected_columns] if selected_columns else ['*']
                    fq_table = f"{target_db}.{target_schema}.{tname}"
                    sql = f'SELECT {", ".join(quoted_cols)} FROM {fq_table} {where_sql} LIMIT 100'
                    try:
                        df = run_query(sql)
                        st.session_state.df = df
                        st.session_state.last_query = f'SELECT {", ".join(quoted_cols)} FROM {fq_table} {where_sql}'
                        st.success(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ {len(df)} ä»¶ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                    except Exception as e:
                        st.error("å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.write(e)

        with right:
            st.subheader("çµæœ")
            if exec_clicked:
                # ç›´è¿‘ã®last_queryãŒã‚ã‚Œã°å†å®Ÿè¡Œï¼ˆLIMIT 100ï¼‰
                if st.session_state.get("last_query"):
                    try:
                        df = run_query(st.session_state["last_query"] + " LIMIT 100")
                        st.session_state.df = df
                        st.success("å†å®Ÿè¡Œã—ã¾ã—ãŸã€‚")
                    except Exception as e:
                        st.error("å†å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.write(e)
                else:
                    st.warning("å®Ÿè¡Œå¯èƒ½ãªã‚¯ã‚¨ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚å·¦å´ã§æ¡ä»¶ã‚’è¨­å®šã—ã¦é©ç”¨ã—ã¦ãã ã•ã„ã€‚")

            if st.session_state.get("last_query"):
                st.markdown("**å®Ÿè¡Œã•ã‚ŒãŸSQLï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰**")
                st.code(st.session_state["last_query"] + " LIMIT 100", language="sql")

            if isinstance(st.session_state.get("df"), pd.DataFrame) and not st.session_state.df.empty:
                st.dataframe(st.session_state.df.head(50), use_container_width=True)

                # ä»¶æ•°ãƒã‚§ãƒƒã‚¯
                with st.container():
                    show_count = st.checkbox("ä»¶æ•°ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰", value=False)
                    if show_count:
                        # last_query ã‹ã‚‰ FROM ... WHERE ... ã‚’ãã®ã¾ã¾ä½¿ã†
                        from_where = st.session_state["last_query"].split("FROM", 1)[1]
                        cnt_sql = "SELECT COUNT(*) AS cnt FROM " + from_where
                        try:
                            total = run_query(cnt_sql).iloc[0, 0]
                            st.markdown(f"<span class='badge badge-run'>ä»¶æ•°: {total} ä»¶</span>", unsafe_allow_html=True)
                        except Exception as e:
                            st.warning("ä»¶æ•°è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                            st.write(e)

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™ãƒ»S3ãƒ»é«˜é€ŸZIP
                def get_full_table():
                    sql = st.session_state.get("last_query", "")
                    return run_query(sql) if sql else pd.DataFrame()
                if dl_clicked:
                    with st.spinner("å…¨ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                        st.session_state.df_for_download = get_full_table()
                    st.success(f"å…¨ä»¶ {len(st.session_state.df_for_download)} ä»¶ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                download_ready_ui(st.session_state.df, selected_table, get_full_table)

            # ä¿å­˜ï¼ˆåå‰å…¥åŠ›ï¼‰
            if save_clicked:
                st.info("ã“ã®SQLã‚’ä¿å­˜ã—ã¾ã™ã€‚ä¿å­˜åã‚’å…¥åŠ›ã—ã¦ã€Œä¿å­˜å®Ÿè¡Œã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
                c1, c2 = st.columns([2, 1])
                with c1:
                    save_name = st.text_input("ä¿å­˜å", key="save_sql_name_ui")
                with c2:
                    if st.button("ä¿å­˜å®Ÿè¡Œ"):
                        if st.session_state.get("last_query"):
                            save_sql_to_log(current_user, st.session_state["last_query"], save_name)
                        else:
                            st.warning("ä¿å­˜å¯¾è±¡ã®SQLãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                show_sql_save_message()

# -------------------------------------------------
# â‘¡ è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«çµåˆ ã‚¿ãƒ–
# -------------------------------------------------
with tabs[1]:
    st.subheader("è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«çµåˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
    if "chain_base_table" not in st.session_state:
        st.session_state.chain_base_table = selected_table if selected_table else ""
    if "chain_steps" not in st.session_state:
        st.session_state.chain_steps = []
    if "chain_preview" not in st.session_state:
        st.session_state.chain_preview = None
    if "chain_total_count" not in st.session_state:
        st.session_state.chain_total_count = None
    if "chain_sql" not in st.session_state:
        st.session_state.chain_sql = None
    if "chain_download_ready" not in st.session_state:
        st.session_state.chain_download_ready = False
    if "chain_df_for_download" not in st.session_state:
        st.session_state.chain_df_for_download = None

    uiL, uiR = st.columns([1, 1.6])

    with uiL:
        base_table = st.selectbox(
            "ä¸»ãƒ†ãƒ¼ãƒ–ãƒ«/ãƒ“ãƒ¥ãƒ¼",
            [""] + all_tables,
            index=([""] + all_tables).index(st.session_state.chain_base_table)
                  if st.session_state.chain_base_table in ([""] + all_tables) else 0
        )
        if base_table != st.session_state.chain_base_table:
            st.session_state.chain_base_table = base_table
            st.session_state.chain_preview = None
            st.session_state.chain_total_count = None
            st.session_state.chain_sql = None
            st.session_state.chain_download_ready = False

        select_all_cols = st.checkbox("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§å…¨åˆ—ã‚’è¡¨ç¤ºï¼ˆé…ããªã‚Šã¾ã™ï¼‰", value=False)

        c_add, c_clear = st.columns(2)
        with c_add:
            if st.button("ï¼‹ çµåˆã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ "):
                st.session_state.chain_steps.append({
                    "right_table": "",
                    "left_key": [],
                    "right_key": [],
                    "how": "INNER"
                })
                st.session_state.chain_preview = None
                st.session_state.chain_total_count = None
                st.session_state.chain_sql = None
                st.session_state.chain_download_ready = False
        with c_clear:
            if st.button("ğŸ§¹ ã™ã¹ã¦ã‚¯ãƒªã‚¢"):
                st.session_state.chain_steps = []
                st.session_state.chain_preview = None
                st.session_state.chain_total_count = None
                st.session_state.chain_sql = None
                st.session_state.chain_download_ready = False

        # ã‚¹ãƒ†ãƒƒãƒ—è¨­å®š
        remove_index = None
        for i, step in enumerate(st.session_state.chain_steps):
            st.markdown(f"**Step {i+1}**")
            lt = base_table if i == 0 else st.session_state.chain_steps[i-1]["right_table"]

            # ã‚­ãƒ¼å€™è£œ
            def cols_of(tbl):
                if not tbl:
                    return []
                dfc = run_query(f"""
                    SELECT COLUMN_NAME
                    FROM {target_db}.INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA='{target_schema}' AND TABLE_NAME='{sanitize_ident(tbl)}'
                    ORDER BY ORDINAL_POSITION
                """)
                return dfc["COLUMN_NAME"].tolist()

            left_cols = cols_of(lt)
            options = [""] + [t for t in all_tables if t != lt] if lt else [""] + all_tables
            step["right_table"] = st.selectbox(
                f"çµåˆå…ˆãƒ†ãƒ¼ãƒ–ãƒ«/ãƒ“ãƒ¥ãƒ¼ (Step {i+1})", options,
                index=options.index(step.get("right_table","")) if step.get("right_table","") in options else 0,
                key=f"rt_{i}"
            )
            step["how"] = st.selectbox(
                "çµåˆæ–¹æ³•", ["INNER","LEFT","RIGHT","FULL"],
                index=["INNER","LEFT","RIGHT","FULL"].index(step.get("how","INNER")),
                key=f"how_{i}"
            )

            right_cols = cols_of(step["right_table"])
            c1, c2, c3 = st.columns([1, 1, .3])
            with c1:
                step["left_key"] = st.multiselect(f"å·¦ã‚­ãƒ¼ï¼ˆ{lt or 'æœªé¸æŠ'}ï¼‰", left_cols, default=step.get("left_key", []), key=f"lk_{i}")
            with c2:
                step["right_key"] = st.multiselect(f"å³ã‚­ãƒ¼ï¼ˆ{step['right_table'] or 'æœªé¸æŠ'}ï¼‰", right_cols, default=step.get("right_key", []), key=f"rk_{i}")
            with c3:
                if st.button("å‰Šé™¤", key=f"rm_{i}"):
                    remove_index = i

            if step["left_key"] and step["right_key"] and len(step["left_key"]) != len(step["right_key"]):
                st.warning("âš  å·¦å³ã®ã‚­ãƒ¼æ•°ãŒä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“ã€‚")

        if remove_index is not None:
            st.session_state.chain_steps.pop(remove_index)
            st.session_state.chain_preview = None
            st.session_state.chain_total_count = None
            st.session_state.chain_sql = None
            st.session_state.chain_download_ready = False

        def build_from_clause(base: str, steps: list) -> str | None:
            if not base:
                return None
            clause = f'{target_db}.{target_schema}.{sanitize_ident(base)}'
            current_left = sanitize_ident(base)
            for s in steps:
                rt = sanitize_ident(s["right_table"]) if s["right_table"] else ""
                lks = s["left_key"]
                rks = s["right_key"]
                how = s["how"]
                if not (rt and lks and rks and how and len(lks)==len(rks)):
                    return None
                on_clause = " AND ".join([f'{current_left}."{lk}" = {rt}."{rk}"' for lk, rk in zip(lks, rks)])
                clause += f' {how} JOIN {target_db}.{target_schema}.{rt} ON {on_clause}'
                current_left = rt
            return clause

        def build_select_clause(base: str, steps: list) -> str:
            tables = [sanitize_ident(base)] + [sanitize_ident(s["right_table"]) for s in steps if s["right_table"]]
            select_cols = []
            for t in tables:
                dfc = run_query(f"""
                    SELECT COLUMN_NAME
                    FROM {target_db}.INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_SCHEMA='{target_schema}' AND TABLE_NAME='{t}'
                    ORDER BY ORDINAL_POSITION
                """)
                cols = dfc["COLUMN_NAME"].tolist()
                if not select_all_cols:
                    cols = cols[:20]
                for c in cols:
                    select_cols.append(f'{t}."{c}" AS "{t}_{c}"')
            return ", ".join(select_cols)

        if st.button("ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦è¡¨ç¤ºï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼100ä»¶ï¼‰"):
            from_clause = build_from_clause(st.session_state.chain_base_table, st.session_state.chain_steps)
            if not from_clause:
                st.error("æœªè¨­å®šã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«/ã‚­ãƒ¼/æ–¹æ³•ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚")
            else:
                select_clause = build_select_clause(st.session_state.chain_base_table, st.session_state.chain_steps)
                sql = f"SELECT {select_clause} FROM {from_clause}"
                try:
                    df_preview = run_query(sql + " LIMIT 100")
                    cnt = run_query(f"SELECT COUNT(*) AS cnt FROM {from_clause}").iloc[0, 0]
                    st.session_state.chain_sql = sql
                    st.session_state.chain_preview = df_preview
                    st.session_state.chain_total_count = cnt
                    st.session_state.chain_download_ready = True
                    st.session_state.show_save_ui = True
                except Exception as e:
                    st.error("SQLå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                    st.write(e)
                    st.session_state.chain_download_ready = False

    with uiR:
        if st.session_state.get("chain_sql"):
            st.markdown("**å®Ÿè¡Œã•ã‚ŒãŸSQL**")
            st.code(st.session_state.chain_sql, language="sql")
        if st.session_state.get("chain_preview") is not None:
            st.dataframe(st.session_state.chain_preview, use_container_width=True)
            st.markdown(f"<span class='badge badge-run'>å…¨ä»¶æ•°: {st.session_state.chain_total_count} ä»¶</span>", unsafe_allow_html=True)

            # DLæº–å‚™ï¼ˆå…¨ä»¶ï¼‰/ é«˜é€ŸZIP / S3
            cA, cB = st.columns([1.2, 1])
            with cA:
                if st.button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™ï¼ˆå…¨ä»¶å–å¾—ï¼‰", key="chain_full"):
                    with st.spinner("å…¨ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                        df_all = run_query(st.session_state.chain_sql)
                        st.session_state.chain_df_for_download = df_all
                    st.success(f"å…¨ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ ({len(st.session_state.chain_df_for_download)}ä»¶)")
            with cB:
                # é«˜é€ŸZIP
                sep_choice = st.radio("åŒºåˆ‡ã‚Š", ["CSV", "TSV"], horizontal=True, key="chain_sep")
                sep = "\t" if sep_choice == "TSV" else ","
                quote_option = st.selectbox("å›²ã„æ–‡å­—", ['"', "'"], index=0, key="chain_quote")
                if st.button("ğŸ“¥ é«˜é€ŸZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="chain_zip"):
                    data = stream_query_to_zip(st.session_state.chain_sql, sep=sep, quotechar=quote_option)
                    st.download_button(
                        "ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹",
                        data=data,
                        file_name=f"{st.session_state.chain_base_table}_{datetime.date.today():%Y%m%d}_server.zip",
                        mime="application/zip",
                        key="chain_zip_dl"
                    )
            # S3
            cS1, cS2 = st.columns(2)
            with cS1:
                if st.button("â¤´ S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰(CSV)", key="chain_s3_csv"):
                    S3_upload(st.session_state.chain_sql, DELIMITER_COMMA, "_join_CSV")
            with cS2:
                if st.button("â¤´ S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰(TSV)", key="chain_s3_tsv"):
                    S3_upload(st.session_state.chain_sql, DELIMITER_TAB, "_join_TSV")

            # DLï¼ˆDFæº–å‚™æ¸ˆã¿ï¼‰
            if st.session_state.get("chain_df_for_download") is not None and not st.session_state.chain_df_for_download.empty:
                show_download_ui(
                    st.session_state.chain_df_for_download,
                    table_name="_".join([st.session_state.chain_base_table] + [s["right_table"] for s in st.session_state.chain_steps if s["right_table"]]),
                    key_prefix="chain_dl"
                )

        # SQLä¿å­˜
        if st.session_state.get("chain_sql"):
            with st.expander("ğŸ’¾ ã“ã®SQLã‚’ä¿å­˜", expanded=False):
                save_name = st.text_input("ä¿å­˜å", key="save_chain_sql_name")
                if st.button("ä¿å­˜å®Ÿè¡Œ", key="save_chain_sql_btn"):
                    save_sql_to_log(current_user, st.session_state.chain_sql, save_name)
                show_sql_save_message()

# -------------------------------------------------
# â‘¢ ä¿å­˜ã—ãŸSQL ã‚¿ãƒ–
# -------------------------------------------------
with tabs[2]:
    st.subheader("ä¿å­˜ã—ãŸSQLã®å†å®Ÿè¡Œ")
    df_sql_log = run_query(f"""
SELECT "LOG_ID", "SQL_NAME", "EXEC_QUERY", "SAVE_DATE"
FROM {target_db}.{target_schema}.SQL_LOG
WHERE "USER_ID" = '{current_user}'
ORDER BY "SAVE_DATE" DESC
""")
    if df_sql_log.empty:
        st.info("ã¾ã ä¿å­˜ã•ã‚ŒãŸSQLã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        selected_id = st.selectbox(
            "å†å®Ÿè¡Œã—ãŸã„SQLã‚’é¸æŠ",
            options=df_sql_log["LOG_ID"],
            format_func=lambda x: f"{df_sql_log.loc[df_sql_log['LOG_ID']==x, 'SQL_NAME'].values[0]} ({df_sql_log.loc[df_sql_log['LOG_ID']==x, 'SAVE_DATE'].values[0]})"
        )
        if st.session_state.get("last_selected_id") != selected_id:
            st.session_state.df_preview = None
            st.session_state.df_for_download = None
            st.session_state.last_selected_id = selected_id

        selected_sql = df_sql_log.loc[df_sql_log["LOG_ID"] == selected_id, "EXEC_QUERY"].values[0]
        cleaned_sql = clean_sql(selected_sql)
        preview_sql = cleaned_sql
        if not re.search(r"(?i)LIMIT\\s+\\d+", selected_sql):
            preview_sql += " LIMIT 100"

        st.markdown("**SQLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**")
        st.code(preview_sql, language="sql")

        c1, c2 = st.columns([1, 2])
        with c1:
            if st.button("ã“ã®SQLã‚’å®Ÿè¡Œï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼100ä»¶ï¼‰"):
                try:
                    df_preview = run_query(preview_sql)
                    st.session_state.df_preview = df_preview
                    st.success(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—å®Œäº† ({len(df_preview)}ä»¶)")
                    st.session_state.last_query = cleaned_sql  # LIMITãªã—ã‚’ä¿æŒ
                except Exception as e:
                    st.error(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        if st.session_state.get("df_preview") is not None:
            st.dataframe(st.session_state.df_preview, use_container_width=True)

            # DLæº–å‚™
            def get_full_data_saved() -> pd.DataFrame:
                sql = st.session_state.get("last_query", "")
                if not sql:
                    st.warning("å®Ÿè¡Œå¯¾è±¡ã®SQLãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    return pd.DataFrame()
                sql_full = clean_sql(remove_limit(sql))
                return run_query(sql_full)

            download_ready_ui(
                df_preview=st.session_state.df_preview,
                table_name=f"savedSQL_{selected_id}",
                sql_all_func=get_full_data_saved
            )

# -------------------------------------------------
# â‘£ ãƒ¡ã‚¿æƒ…å ± ã‚¿ãƒ–
# -------------------------------------------------
with tabs[3]:
    st.subheader("ãƒ¡ã‚¿æƒ…å ±")
    if not selected_table:
        st.info("ã‚³ãƒãƒ³ãƒ‰ãƒãƒ¼ã§ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ãƒ“ãƒ¥ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    else:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚³ãƒ¡ãƒ³ãƒˆ
        df_comment = run_query(f"""
            SELECT COMMENT
            FROM {target_db}.INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA='{target_schema}' AND TABLE_NAME='{sanitize_ident(selected_table)}'
        """)
        comment = df_comment.iloc[0,0] if (not df_comment.empty and df_comment.iloc[0,0]) else "(èª¬æ˜ãªã—)"
        st.markdown(f"**ãƒ†ãƒ¼ãƒ–ãƒ«èª¬æ˜:** {comment}")

        # ã‚«ãƒ©ãƒ è¾æ›¸ï¼ˆCODE_Mï¼‰
        st.markdown("**ã‚«ãƒ©ãƒ è¾æ›¸ï¼ˆCODE_Mï¼‰ã‚µãƒ³ãƒ—ãƒ«**")
        df_cols = run_query(f"""
            SELECT COLUMN_NAME, DATA_TYPE, COMMENT
            FROM {target_db}.INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA='{target_schema}' AND TABLE_NAME='{sanitize_ident(selected_table)}'
            ORDER BY ORDINAL_POSITION
        """)
        cols = df_cols["COLUMN_NAME"].tolist()
        if cols:
            col_list = "', '".join(cols)
            code_df = run_query(f"""
SELECT "ã‚«ãƒ©ãƒ å", "ã‚³ãƒ¼ãƒ‰å€¤", "ã‚³ãƒ¼ãƒ‰å€¤åç§°"
FROM {target_db}.{target_schema}.CODE_M
WHERE "ã‚«ãƒ©ãƒ å" IN ('{col_list}')
""")
            if code_df.empty:
                st.caption("CODE_M ã«å¯¾å¿œã‚¨ãƒ³ãƒˆãƒªã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.dataframe(code_df.head(200), use_container_width=True)
